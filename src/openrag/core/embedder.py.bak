"""Embedding model wrapper using sentence-transformers."""

from typing import Any

from sentence_transformers import SentenceTransformer

from ..utils.logger import setup_logger

logger = setup_logger(__name__)


class EmbeddingError(Exception):
    """Exception raised during embedding operations."""

    pass


class EmbeddingModel:
    """
    Wrapper for sentence-transformers embedding models.

    Provides a simple interface for generating embeddings from text with
    support for batch processing and error handling.
    """

    def __init__(self, model_name: str = "all-mpnet-base-v2"):
        """
        Initialize the embedding model.

        Args:
            model_name: Name of the sentence-transformers model to use

        Raises:
            EmbeddingError: If model loading fails

        Note:
            Model will be downloaded on first use if not cached (~420MB for all-mpnet-base-v2).
            Models are cached in ~/.cache/torch/sentence_transformers/
        """
        self.model_name = model_name

        try:
            logger.info(f"Loading embedding model: {model_name}")
            self.model = SentenceTransformer(model_name)
            logger.info(f"Successfully loaded model: {model_name}")
        except Exception as e:
            error_msg = (
                f"Failed to load embedding model '{model_name}'. "
                f"Error: {str(e)}. "
                "Please check model name and internet connection."
            )
            logger.error(error_msg)
            raise EmbeddingError(error_msg) from e

        # Store model metadata
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        logger.info(f"Model embedding dimension: {self.embedding_dim}")

    def embed_texts(self, texts: list[str], batch_size: int = 32) -> list[list[float]]:
        """
        Generate embeddings for a list of texts.

        Args:
            texts: List of text strings to embed
            batch_size: Batch size for processing (default: 32)

        Returns:
            List of embedding vectors (each is a list of floats)

        Raises:
            EmbeddingError: If embedding generation fails

        Example:
            >>> model = EmbeddingModel("all-MiniLM-L6-v2")
            >>> embeddings = model.embed_texts(["Hello world", "Goodbye world"])
            >>> len(embeddings)
            2
            >>> len(embeddings[0])
            384
        """
        if not texts:
            logger.warning("Empty text list provided to embed_texts")
            return []

        try:
            logger.debug(f"Generating embeddings for {len(texts)} texts")

            # Generate embeddings (returns numpy array)
            embeddings_array = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=False,
                convert_to_numpy=True,
            )

            # Convert to list of lists for JSON serialization
            embeddings_list: list[list[float]] = embeddings_array.tolist()

            logger.debug(
                f"Generated {len(embeddings_list)} embeddings of dimension {len(embeddings_list[0])}"
            )

            return embeddings_list

        except Exception as e:
            error_msg = f"Failed to generate embeddings: {str(e)}"
            logger.error(error_msg)
            raise EmbeddingError(error_msg) from e

    def embed_text(self, text: str) -> list[float]:
        """
        Generate embedding for a single text.

        Args:
            text: Text string to embed

        Returns:
            Embedding vector as list of floats

        Raises:
            EmbeddingError: If embedding generation fails
        """
        embeddings = self.embed_texts([text])
        return embeddings[0]

    def get_embedding_dimension(self) -> int:
        """
        Get the dimensionality of embeddings produced by this model.

        Returns:
            Embedding dimension (e.g., 384 for MiniLM, 768 for mpnet)
        """
        return self.embedding_dim

    def get_model_info(self) -> dict[str, Any]:
        """
        Get information about the loaded model.

        Returns:
            Dictionary with model metadata
        """
        return {
            "model_name": self.model_name,
            "embedding_dimension": self.embedding_dim,
            "max_seq_length": self.model.max_seq_length,
        }
